{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shafiqulrehman/AlzheimerCode/blob/main/DARWIN_Different_Ensemble_types.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying different Ensemble models**\n",
        "1. Majority voting\n",
        "2. Bagging (Bootstrap Aggregating)\n",
        "3. Boosting\n",
        "4. Gradient Boosted Decision Trees (GBDT)\n",
        "5. Bayesian Model Averaging\n",
        "6. Bootstrap Aggregating for Time Series (BATS)\n",
        "7. Stacking\n"
      ],
      "metadata": {
        "id": "zGbARcD2TaZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vXTzfO51uJs",
        "outputId": "f29b7055-a1c5-43f4-9a9c-942972022bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9143\n",
            "Sensitivity (Recall): 0.9000\n",
            "Specificity: 0.9333\n",
            "Precision: 0.9474\n",
            "TPR (True Positive Rate): 0.9000\n",
            "FPR (False Positive Rate): 0.0667\n",
            "F1-score: 0.9231\n",
            "MCC (Matthews Correlation Coefficient): 0.8278\n",
            "Cohen's Kappa: 0.8264\n",
            "AUC-ROC: 0.9167\n"
          ]
        }
      ],
      "source": [
        "########### Ensemble model using majority voting  #########\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_selection import RFE, SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef, cohen_kappa_score\n",
        "\n",
        "# Read the dataset from CSV\n",
        "def read_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "# Preprocess the dataset\n",
        "def preprocess_data(df):\n",
        "    # Assuming the last column is the class label\n",
        "    X = df.iloc[:, :-1]  # Features\n",
        "    y = df.iloc[:, -1]   # Class labels\n",
        "\n",
        "    # Convert class labels to numerical values (if needed)\n",
        "    y = y.map({'H': 0, 'P': 1})\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Apply Recursive Feature Elimination (RFE) and train classifier\n",
        "def rfe_and_train(X_train, y_train, k, classifier):\n",
        "    model = classifier(random_state=42)\n",
        "    rfe = RFE(estimator=model, n_features_to_select=k)\n",
        "    X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "    model.fit(X_train_rfe, y_train)\n",
        "    return model, rfe\n",
        "\n",
        "# Apply SelectKBest with ANOVA and train classifier\n",
        "def selectkbest_and_train(X_train, y_train, k, classifier):\n",
        "    skb = SelectKBest(f_classif, k=k)\n",
        "    X_train_skb = skb.fit_transform(X_train, y_train)\n",
        "    model = classifier()\n",
        "    model.fit(X_train_skb, y_train)\n",
        "    return model, skb\n",
        "\n",
        "# Combine predictions using majority voting\n",
        "def combine_predictions(predictions):\n",
        "    # Use mode to get the most common prediction for each sample\n",
        "    majority_votes = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=predictions)\n",
        "    return majority_votes\n",
        "\n",
        "# Evaluate the model on test data\n",
        "def evaluate_model(model, X_test, y_test, selector):\n",
        "    X_test_selected = selector.transform(X_test)\n",
        "    y_pred = model.predict(X_test_selected)\n",
        "    return y_pred\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sensitivity = recall_score(y_true, y_pred)\n",
        "    specificity = tn / (tn + fp)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    tpr = sensitivity\n",
        "    fpr = 1 - specificity\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    auc_roc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "    return acc, sensitivity, specificity, precision, tpr, fpr, f1, mcc, kappa, auc_roc\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Path to your CSV file\n",
        "    file_path = '/content/data_null_outlier_free_normalized_corr_90.csv'\n",
        "\n",
        "    # Read the dataset\n",
        "    df = read_csv(file_path)\n",
        "\n",
        "    # Preprocess the data\n",
        "    X, y = preprocess_data(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Set the value of k for Extra Trees classifier\n",
        "    k_et = 200\n",
        "\n",
        "    # Apply RFE and train the Extra Trees classifier\n",
        "    model_et, rfe_et = rfe_and_train(X_train, y_train, k_et, ExtraTreesClassifier)\n",
        "\n",
        "    # Set the value of k for Random Forest classifier\n",
        "    k_rf = 250\n",
        "\n",
        "    # Apply RFE and train the Random Forest classifier\n",
        "    model_rf, rfe_rf = rfe_and_train(X_train, y_train, k_rf, RandomForestClassifier)\n",
        "\n",
        "    # Set the value of k for Logistic Regression classifier\n",
        "    k_lr = 50\n",
        "\n",
        "    # Apply RFE and train the Logistic Regression classifier\n",
        "    model_lr, rfe_lr = rfe_and_train(X_train, y_train, k_lr, LogisticRegression)\n",
        "\n",
        "    # Set the value of k for Gaussian Naive Bayes classifier\n",
        "    k_gnb = 320\n",
        "\n",
        "    # Apply SelectKBest with ANOVA and train the Gaussian Naive Bayes classifier\n",
        "    model_gnb, skb_gnb = selectkbest_and_train(X_train, y_train, k_gnb, GaussianNB)\n",
        "\n",
        "    # Set the value of k for SVM classifier\n",
        "    k_svm = 150\n",
        "\n",
        "    # Apply SelectKBest with ANOVA and train the SVM classifier\n",
        "    model_svm, skb_svm = selectkbest_and_train(X_train, y_train, k_svm, SVC)\n",
        "\n",
        "    # Set the value of k for MLP classifier\n",
        "    k_mlp = 290\n",
        "\n",
        "    # Apply SelectKBest with ANOVA and train the MLP classifier\n",
        "    model_mlp, skb_mlp = selectkbest_and_train(X_train, y_train, k_mlp, MLPClassifier)\n",
        "\n",
        "    # Set the value of k for XGB classifier\n",
        "    k_xgb = 60\n",
        "\n",
        "    # Apply RFE and train the XGB classifier\n",
        "    model_xgb, rfe_xgb = rfe_and_train(X_train, y_train, k_xgb, XGBClassifier)\n",
        "\n",
        "    # Evaluate each model on the test set\n",
        "    predictions_et = evaluate_model(model_et, X_test, y_test, rfe_et)\n",
        "    predictions_rf = evaluate_model(model_rf, X_test, y_test, rfe_rf)\n",
        "    predictions_lr = evaluate_model(model_lr, X_test, y_test, rfe_lr)\n",
        "    predictions_gnb = evaluate_model(model_gnb, X_test, y_test, skb_gnb)\n",
        "    predictions_svm = evaluate_model(model_svm, X_test, y_test, skb_svm)\n",
        "    predictions_mlp = evaluate_model(model_mlp, X_test, y_test, skb_mlp)\n",
        "    predictions_xgb = evaluate_model(model_xgb, X_test, y_test, rfe_xgb)\n",
        "\n",
        "    # Combine predictions using majority voting\n",
        "    combined_predictions = combine_predictions([predictions_et, predictions_rf, predictions_svm, predictions_xgb, predictions_mlp, predictions_gnb  , predictions_lr ])\n",
        "    # predictions_xgb, predictions_mlp,, predictions_gnb  , predictions_lr\n",
        "    # Calculate evaluation metrics for the combined predictions\n",
        "    acc, sensitivity, specificity, precision, tpr, fpr, f1, mcc, kappa, auc_roc = calculate_metrics(y_test, combined_predictions)\n",
        "\n",
        "    # Print the results\n",
        "    print(f'Accuracy: {acc:.4f}')\n",
        "    print(f'Sensitivity (Recall): {sensitivity:.4f}')\n",
        "    print(f'Specificity: {specificity:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'TPR (True Positive Rate): {tpr:.4f}')\n",
        "    print(f'FPR (False Positive Rate): {fpr:.4f}')\n",
        "    print(f'F1-score: {f1:.4f}')\n",
        "    print(f'MCC (Matthews Correlation Coefficient): {mcc:.4f}')\n",
        "    print(f'Cohen\\'s Kappa: {kappa:.4f}')\n",
        "    print(f'AUC-ROC: {auc_roc:.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########### Bagging Ensemble Model #########\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_selection import RFE, SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef, cohen_kappa_score\n",
        "\n",
        "# Read the dataset from CSV\n",
        "def read_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "# Preprocess the dataset\n",
        "def preprocess_data(df):\n",
        "    # Assuming the last column is the class label\n",
        "    X = df.iloc[:, :-1]  # Features\n",
        "    y = df.iloc[:, -1]   # Class labels\n",
        "\n",
        "    # Convert class labels to numerical values (if needed)\n",
        "    y = y.map({'H': 0, 'P': 1})\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Apply Recursive Feature Elimination (RFE) and train classifier\n",
        "def rfe_and_train(X_train, y_train, k, classifier):\n",
        "    model = classifier(random_state=42)\n",
        "    rfe = RFE(estimator=model, n_features_to_select=k)\n",
        "    X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "    model.fit(X_train_rfe, y_train)\n",
        "    return model, rfe\n",
        "\n",
        "# Apply SelectKBest with ANOVA and train classifier\n",
        "def selectkbest_and_train(X_train, y_train, k, classifier):\n",
        "    skb = SelectKBest(f_classif, k=k)\n",
        "    X_train_skb = skb.fit_transform(X_train, y_train)\n",
        "    model = classifier()\n",
        "    model.fit(X_train_skb, y_train)\n",
        "    return model, skb\n",
        "\n",
        "# Evaluate the model on test data\n",
        "def evaluate_model(model, X_test, y_test, selector):\n",
        "    X_test_selected = selector.transform(X_test)\n",
        "    y_pred = model.predict(X_test_selected)\n",
        "    return y_pred\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sensitivity = recall_score(y_true, y_pred)\n",
        "    specificity = tn / (tn + fp)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    tpr = sensitivity\n",
        "    fpr = 1 - specificity\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    auc_roc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "    return acc, sensitivity, specificity, precision, tpr, fpr, f1, mcc, kappa, auc_roc\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Path to your CSV file\n",
        "    file_path = '/content/data_null_outlier_free_normalized_corr_90.csv'\n",
        "\n",
        "    # Read the dataset\n",
        "    df = read_csv(file_path)\n",
        "\n",
        "    # Preprocess the data\n",
        "    X, y = preprocess_data(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Set the value of k for Extra Trees classifier\n",
        "    k_et = 200\n",
        "\n",
        "    # Apply RFE and train the Extra Trees classifier\n",
        "    model_et, rfe_et = rfe_and_train(X_train, y_train, k_et, ExtraTreesClassifier)\n",
        "\n",
        "    # Set the value of k for Random Forest classifier\n",
        "    k_rf = 250\n",
        "\n",
        "    # Apply RFE and train the Random Forest classifier\n",
        "    model_rf, rfe_rf = rfe_and_train(X_train, y_train, k_rf, RandomForestClassifier)\n",
        "\n",
        "    # Set the value of k for Logistic Regression classifier\n",
        "    k_lr = 50\n",
        "\n",
        "    # Apply RFE and train the Logistic Regression classifier\n",
        "    model_lr, rfe_lr = rfe_and_train(X_train, y_train, k_lr, LogisticRegression)\n",
        "\n",
        "    # Set the value of k for Gaussian Naive Bayes classifier\n",
        "    k_gnb = 320\n",
        "\n",
        "    # Apply SelectKBest with ANOVA and train the Gaussian Naive Bayes classifier\n",
        "    model_gnb, skb_gnb = selectkbest_and_train(X_train, y_train, k_gnb, GaussianNB)\n",
        "\n",
        "    # Set the value of k for SVM classifier\n",
        "    k_svm = 150\n",
        "\n",
        "    # Apply SelectKBest with ANOVA and train the SVM classifier\n",
        "    model_svm, skb_svm = selectkbest_and_train(X_train, y_train, k_svm, SVC)\n",
        "\n",
        "    # Set the value of k for MLP classifier\n",
        "    k_mlp = 290\n",
        "\n",
        "    # Apply SelectKBest with ANOVA and train the MLP classifier\n",
        "    model_mlp, skb_mlp = selectkbest_and_train(X_train, y_train, k_mlp, MLPClassifier)\n",
        "\n",
        "    # Set the value of k for XGB classifier\n",
        "    k_xgb = 60\n",
        "\n",
        "    # Apply RFE and train the XGB classifier\n",
        "    model_xgb, rfe_xgb = rfe_and_train(X_train, y_train, k_xgb, XGBClassifier)\n",
        "\n",
        "    # Initialize Bagging classifier for each base model\n",
        "    base_models = [\n",
        "        ('ET', model_et),\n",
        "        ('RF', model_rf),\n",
        "        ('LR', model_lr),\n",
        "        ('GNB', model_gnb),\n",
        "        ('SVM', model_svm),\n",
        "        ('MLP', model_mlp),\n",
        "        ('XGB', model_xgb)\n",
        "    ]\n",
        "    bagging_ensemble = BaggingClassifier(base_estimator=None, n_estimators=10, random_state=42)\n",
        "\n",
        "    # Fit the Bagging ensemble\n",
        "    bagging_ensemble.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the Bagging ensemble on the test set\n",
        "    y_pred_bagging = bagging_ensemble.predict(X_test)\n",
        "\n",
        "    # Calculate evaluation metrics for the Bagging ensemble\n",
        "    acc, sensitivity, specificity, precision, tpr, fpr, f1, mcc, kappa, auc_roc = calculate_metrics(y_test, y_pred_bagging)\n",
        "\n",
        "    # Print the results\n",
        "    print(f'Accuracy: {acc:.4f}')\n",
        "    print(f'Sensitivity (Recall): {sensitivity:.4f}')\n",
        "    print(f'Specificity: {specificity:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'TPR (True Positive Rate): {tpr:.4f}')\n",
        "    print(f'FPR (False Positive Rate): {fpr:.4f}')\n",
        "    print(f'F1-score: {f1:.4f}')\n",
        "    print(f'MCC (Matthews Correlation Coefficient): {mcc:.4f}')\n",
        "    print(f'Cohen\\'s Kappa: {kappa:.4f}')\n",
        "    print(f'AUC-ROC: {auc_roc:.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7_1PFaGVmpQ",
        "outputId": "a4bcc511-6beb-4c1b-ff50-9b9d4fa01996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8286\n",
            "Sensitivity (Recall): 0.7500\n",
            "Specificity: 0.9333\n",
            "Precision: 0.9375\n",
            "TPR (True Positive Rate): 0.7500\n",
            "FPR (False Positive Rate): 0.0667\n",
            "F1-score: 0.8333\n",
            "MCC (Matthews Correlation Coefficient): 0.6788\n",
            "Cohen's Kappa: 0.6613\n",
            "AUC-ROC: 0.8417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########### Boosting Ensemble Model #########\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import RFE, SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef, cohen_kappa_score\n",
        "\n",
        "# Read the dataset from CSV\n",
        "def read_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "# Preprocess the dataset\n",
        "def preprocess_data(df):\n",
        "    # Assuming the last column is the class label\n",
        "    X = df.iloc[:, :-1]  # Features\n",
        "    y = df.iloc[:, -1]   # Class labels\n",
        "\n",
        "    # Convert class labels to numerical values (if needed)\n",
        "    y = y.map({'H': 0, 'P': 1})\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Apply Recursive Feature Elimination (RFE) and train classifier\n",
        "def rfe_and_train(X_train, y_train, k, classifier):\n",
        "    model = classifier(random_state=42)\n",
        "    rfe = RFE(estimator=model, n_features_to_select=k)\n",
        "    X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "    model.fit(X_train_rfe, y_train)\n",
        "    return model, rfe\n",
        "\n",
        "# Apply SelectKBest with ANOVA and train classifier\n",
        "def selectkbest_and_train(X_train, y_train, k, classifier):\n",
        "    skb = SelectKBest(f_classif, k=k)\n",
        "    X_train_skb = skb.fit_transform(X_train, y_train)\n",
        "    model = classifier()\n",
        "    model.fit(X_train_skb, y_train)\n",
        "    return model, skb\n",
        "\n",
        "# Evaluate the model on test data\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    return y_pred\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sensitivity = recall_score(y_true, y_pred)\n",
        "    specificity = tn / (tn + fp)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    tpr = sensitivity\n",
        "    fpr = 1 - specificity\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    auc_roc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "    return acc, sensitivity, specificity, precision, tpr, fpr, f1, mcc, kappa, auc_roc\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Path to your CSV file\n",
        "    file_path = '/content/data_null_outlier_free_normalized_corr_90.csv'\n",
        "\n",
        "    # Read the dataset\n",
        "    df = read_csv(file_path)\n",
        "\n",
        "    # Preprocess the data\n",
        "    X, y = preprocess_data(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Set the value of k for Extra Trees classifier\n",
        "    k_et = 200\n",
        "\n",
        "    # Apply RFE and train the Extra Trees classifier\n",
        "    model_et, rfe_et = rfe_and_train(X_train, y_train, k_et, ExtraTreesClassifier)\n",
        "\n",
        "    # Set the value of k for Random Forest classifier\n",
        "    k_rf = 250\n",
        "\n",
        "    # Apply RFE and train the Random Forest classifier\n",
        "    model_rf, rfe_rf = rfe_and_train(X_train, y_train, k_rf, RandomForestClassifier)\n",
        "\n",
        "    # Set the value of k for Logistic Regression classifier\n",
        "    k_lr = 50\n",
        "\n",
        "    # Apply RFE and train the Logistic Regression classifier\n",
        "    model_lr, rfe_lr = rfe_and_train(X_train, y_train, k_lr, LogisticRegression)\n",
        "\n",
        "    # Set the value of k for Gaussian Naive Bayes classifier\n",
        "    k_gnb = 320\n",
        "\n",
        "    # Apply SelectKBest with ANOVA and train the Gaussian Naive Bayes classifier\n",
        "    model_gnb, skb_gnb = selectkbest_and_train(X_train, y_train, k_gnb, GaussianNB)\n",
        "\n",
        "    # Set the value of k for Decision Tree classifier\n",
        "    k_dt = 60\n",
        "\n",
        "    # Apply RFE and train the Decision Tree classifier\n",
        "    model_dt, rfe_dt = rfe_and_train(X_train, y_train, k_dt, DecisionTreeClassifier)\n",
        "\n",
        "    # Initialize AdaBoost classifier\n",
        "    base_estimators = [model_et, model_rf, model_lr, model_gnb, model_dt]\n",
        "    adaboost_ensemble = AdaBoostClassifier(base_estimator=None, n_estimators=50, random_state=42)\n",
        "\n",
        "    # Fit the AdaBoost ensemble\n",
        "    adaboost_ensemble.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the AdaBoost ensemble on the test set\n",
        "    y_pred_adaboost = evaluate_model(adaboost_ensemble, X_test, y_test)\n",
        "\n",
        "    # Calculate evaluation metrics for the AdaBoost ensemble\n",
        "    acc, sensitivity, specificity, precision, tpr, fpr, f1, mcc, kappa, auc_roc = calculate_metrics(y_test, y_pred_adaboost)\n",
        "\n",
        "    # Print the results\n",
        "    print(f'Accuracy: {acc:.4f}')\n",
        "    print(f'Sensitivity (Recall): {sensitivity:.4f}')\n",
        "    print(f'Specificity: {specificity:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'TPR (True Positive Rate): {tpr:.4f}')\n",
        "    print(f'FPR (False Positive Rate): {fpr:.4f}')\n",
        "    print(f'F1-score: {f1:.4f}')\n",
        "    print(f'MCC (Matthews Correlation Coefficient): {mcc:.4f}')\n",
        "    print(f'Cohen\\'s Kappa: {kappa:.4f}')\n",
        "    print(f'AUC-ROC: {auc_roc:.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "pb0gBojii6EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GceI-9XHirif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## Ensemble model using Stacking ##########\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_selection import RFE, SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef, cohen_kappa_score\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# Read the dataset from CSV\n",
        "def read_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "# Preprocess the dataset\n",
        "def preprocess_data(df):\n",
        "    # Assuming the last column is the class label\n",
        "    X = df.iloc[:, :-1]  # Features\n",
        "    y = df.iloc[:, -1]   # Class labels\n",
        "\n",
        "    # Convert class labels to numerical values (if needed)\n",
        "    y = y.map({'H': 0, 'P': 1})\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Apply Recursive Feature Elimination (RFE) and train classifier\n",
        "def rfe_and_train(X_train, y_train, k, classifier):\n",
        "    model = classifier(random_state=42)\n",
        "    rfe = RFE(estimator=model, n_features_to_select=k)\n",
        "    X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "    model.fit(X_train_rfe, y_train)\n",
        "    return model, rfe\n",
        "\n",
        "# Apply SelectKBest with ANOVA and train classifier\n",
        "def selectkbest_and_train(X_train, y_train, k, classifier):\n",
        "    skb = SelectKBest(f_classif, k=k)\n",
        "    X_train_skb = skb.fit_transform(X_train, y_train)\n",
        "    model = classifier()\n",
        "    model.fit(X_train_skb, y_train)\n",
        "    return model, skb\n",
        "\n",
        "# Combine predictions using majority voting\n",
        "def combine_predictions(predictions):\n",
        "    # Use mode to get the most common prediction for each sample\n",
        "    majority_votes = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=predictions)\n",
        "    return majority_votes\n",
        "\n",
        "# Evaluate the model on test data\n",
        "def evaluate_model(model, X_test, y_test, selector):\n",
        "    X_test_selected = selector.transform(X_test)\n",
        "    y_pred = model.predict(X_test_selected)\n",
        "    return y_pred\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sensitivity = recall_score(y_true, y_pred)\n",
        "    specificity = tn / (tn + fp)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    tpr = sensitivity\n",
        "    fpr = 1 - specificity\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    auc_roc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "    return acc, sensitivity, specificity, precision, tpr, fpr, f1, mcc, kappa, auc_roc\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Path to your CSV file\n",
        "    #file_path = '/content/data_null_outlier_free_normalized.csv'\n",
        "    file_path = '/content/data_null_outlier_free_normalized_corr_90.csv'\n",
        "    # Read the dataset\n",
        "    df = read_csv(file_path)\n",
        "\n",
        "    # Preprocess the data\n",
        "    X, y = preprocess_data(df)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Set the value of k for Extra Trees classifier\n",
        "    k_et = 200\n",
        "\n",
        "    # Apply RFE and train the Extra Trees classifier\n",
        "    model_et, rfe_et = rfe_and_train(X_train, y_train, k_et, ExtraTreesClassifier)\n",
        "\n",
        "    # Set the value of k for Random Forest classifier\n",
        "    k_rf = 250\n",
        "\n",
        "    # Apply RFE and train the Random Forest classifier\n",
        "    model_rf, rfe_rf = rfe_and_train(X_train, y_train, k_rf, RandomForestClassifier)\n",
        "\n",
        "    # Set the value of k for SVM classifier\n",
        "    k_svm = 150\n",
        "\n",
        "    # Apply SelectKBest with ANOVA and train the SVM classifier\n",
        "    model_svm, skb_svm = selectkbest_and_train(X_train, y_train, k_svm, SVC)\n",
        "\n",
        "    # Combine predictions from base-level models\n",
        "    base_level_predictions = np.column_stack([evaluate_model(model_et, X_test, y_test, rfe_et),\n",
        "                                              evaluate_model(model_rf, X_test, y_test, rfe_rf),\n",
        "                                              evaluate_model(model_svm, X_test, y_test, skb_svm)])\n",
        "\n",
        "    # Train the meta-label classifier (Logistic Regression)\n",
        "    meta_classifier = LogisticRegression(random_state=42)\n",
        "    stacking_model = StackingClassifier(estimators=[\n",
        "        ('et', ExtraTreesClassifier(random_state=42)),\n",
        "        ('rf', RandomForestClassifier(random_state=42)),\n",
        "        ('svm', SVC(random_state=42)),\n",
        "    ], final_estimator=meta_classifier)\n",
        "\n",
        "    # Fit the stacking model on base-level predictions\n",
        "    stacking_model.fit(base_level_predictions, y_test)\n",
        "\n",
        "    # Make predictions using the stacking model\n",
        "    stacking_predictions = stacking_model.predict(base_level_predictions)\n",
        "\n",
        "    # Calculate evaluation metrics for the stacking predictions\n",
        "    acc, sensitivity, specificity, precision, tpr, fpr, f1, mcc, kappa, auc_roc = calculate_metrics(y_test, stacking_predictions)\n",
        "\n",
        "    # Print the results\n",
        "    print('Results after stacking:')\n",
        "    print(f'Accuracy: {acc:.4f}')\n",
        "    print(f'Sensitivity (Recall): {sensitivity:.4f}')\n",
        "    print(f'Specificity: {specificity:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'TPR (True Positive Rate): {tpr:.4f}')\n",
        "    print(f'FPR (False Positive Rate): {fpr:.4f}')\n",
        "    print(f'F1-score: {f1:.4f}')\n",
        "    print(f'MCC (Matthews Correlation Coefficient): {mcc:.4f}')\n",
        "    print(f'Cohen\\'s Kappa: {kappa:.4f}')\n",
        "    print(f'AUC-ROC: {auc_roc:.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yulpc1qlQBZf",
        "outputId": "8e31c42c-b36a-489f-ae0d-19136f1ff790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after stacking:\n",
            "Accuracy: 0.9714\n",
            "Sensitivity (Recall): 0.9500\n",
            "Specificity: 1.0000\n",
            "Precision: 1.0000\n",
            "TPR (True Positive Rate): 0.9500\n",
            "FPR (False Positive Rate): 0.0000\n",
            "F1-score: 0.9744\n",
            "MCC (Matthews Correlation Coefficient): 0.9437\n",
            "Cohen's Kappa: 0.9421\n",
            "AUC-ROC: 0.9750\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}