{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shafiqulrehman/AlzheimerCode/blob/main/Alzheimer_Dara_Pre_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Data pre-processing pipe-line**\n",
        "\n",
        "\n",
        "\n",
        "1.   Null value removal\n",
        "2.   Detection aof outliers by using IQR, and replacing them by mode values.\n",
        "3. Performing Z-score normalization to bring the features in a single scale\n",
        "4. Removing highly correlated features(>90%)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kDBrZ2UUG79R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Removing null values\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Read the CSV file\n",
        "input_file = '/content/data.csv'\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Step 2: Delete the first column\n",
        "df = df.iloc[:, 1:]\n",
        "\n",
        "# Step 3: Check for null values and replace them with the mode\n",
        "null_columns = df.columns[df.isnull().any()]\n",
        "for column in null_columns:\n",
        "    mode_value = df[column].mode()[0]  # Get the mode value of the column\n",
        "    df[column].fillna(mode_value, inplace=True)\n",
        "\n",
        "# Step 4: Store the filtered dataset in a new CSV file\n",
        "output_file = 'data_free_null.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Filtered data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY7KCoR_HB1x",
        "outputId": "48d12a77-115a-473a-edee-56c797d8c649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered data saved to data_free_null.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Detecting the outliers in each feature and replacing them by the mode value of that feature\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Suppressing the warning\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "\n",
        "# Step 1: Read the CSV file\n",
        "input_file = '/content/data_free_null.csv'\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Step 2: Define a function to replace outliers with the mode of the column\n",
        "def replace_outliers_with_mode(column):\n",
        "    Q1 = column.quantile(0.25)\n",
        "    Q3 = column.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Replace outliers with the mode of the column\n",
        "    column[(column < lower_bound) | (column > upper_bound)] = column.mode().iloc[0]\n",
        "    return column\n",
        "\n",
        "# Step 3: Loop through numeric columns (except the last one) and replace outliers with the mode\n",
        "numeric_columns = df.iloc[:, :-1].select_dtypes(include=[np.number]).columns\n",
        "for column in numeric_columns:\n",
        "    df[column] = replace_outliers_with_mode(df[column])\n",
        "\n",
        "# Step 4: Store the modified dataset in a new CSV file\n",
        "output_file = 'data_free_null_outlier.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Outliers replaced and filtered data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOKXeOsBH3sY",
        "outputId": "90eea951-85d5-4b81-87d8-87494b4032e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outliers replaced and filtered data saved to data_free_null_outlier.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Read the CSV file\n",
        "input_file = 'data_free_null_outlier.csv'\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Step 2: Exclude the last column (class label)\n",
        "data = df.iloc[:, :-1]\n",
        "\n",
        "# Step 3: Apply Z-score normalization to standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "normalized_data = scaler.fit_transform(data)\n",
        "\n",
        "# Step 4: Create a new DataFrame with the standardized data\n",
        "normalized_df = pd.DataFrame(normalized_data, columns=data.columns)\n",
        "\n",
        "# Step 5: Concatenate the standardized data with the last column (class label)\n",
        "normalized_df = pd.concat([normalized_df, df.iloc[:, -1]], axis=1)\n",
        "\n",
        "# Step 6: Store the standardized dataset in a new CSV file\n",
        "output_file = 'data_free_null_outlier_normalized.csv'\n",
        "normalized_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Z-score normalized data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVHQtHVtKaSL",
        "outputId": "1b586437-e0ce-4ac3-8050-687d1a320ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z-score normalized data saved to data_free_null_outlier_normalized.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Read the CSV file\n",
        "csv_file = '/content/data_free_null_outlier_normalized.csv'  # Replace with your CSV file path\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Step 2: Compute the correlation matrix, ignoring the last column (class label)\n",
        "correlation_matrix = df.iloc[:, :-1].corr()\n",
        "\n",
        "# Step 3: Find highly correlated columns\n",
        "highly_correlated = set()\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > 0.90:\n",
        "            column_i = correlation_matrix.columns[i]\n",
        "            column_j = correlation_matrix.columns[j]\n",
        "            # Add both columns to the set of highly correlated columns\n",
        "            highly_correlated.add(column_i)\n",
        "            highly_correlated.add(column_j)\n",
        "\n",
        "# Step 4: Remove one of the columns from each highly correlated pair\n",
        "df_filtered = df.drop(columns=list(highly_correlated))\n",
        "\n",
        "# Step 5: Save the filtered DataFrame to a new CSV file\n",
        "filtered_csv_file = 'data_null_outlier_free_normalized_filtered_data.csv'  # Replace with your desired output file path\n",
        "df_filtered.to_csv(filtered_csv_file, index=False)\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "print(\"Filtered DataFrame:\")\n",
        "print(df_filtered.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1v4j-TmBOT5",
        "outputId": "3e588792-43b7-4eb0-b39d-4b24a4bcaf60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered DataFrame:\n",
            "   air_time1  disp_index1  gmrt_in_air1  max_x_extension1  max_y_extension1  \\\n",
            "0   1.046526     1.145182     -0.982342         -1.352980         -0.270435   \n",
            "1  -1.166959     2.515942     -1.023595          0.111876         -0.020048   \n",
            "2  -0.160401     0.283561     -0.161713          1.381949         -0.774361   \n",
            "3  -0.381985     0.283561      0.887062          0.235107          0.712190   \n",
            "4  -0.297123    -1.063701      0.049315         -1.293352         -1.449206   \n",
            "\n",
            "   mean_acc_on_paper1  mean_gmrt1  mean_jerk_on_paper1  mean_speed_in_air1  \\\n",
            "0            1.833678   -1.177858             0.760912           -0.960257   \n",
            "1           -0.375818   -1.224259            -0.738144           -0.967027   \n",
            "2            0.734186   -0.159928             0.304085            0.055691   \n",
            "3            0.221535    0.622423             0.633182            1.172486   \n",
            "4           -0.368987   -0.334366            -0.189763            0.335069   \n",
            "\n",
            "   num_of_pendown1  ...  max_x_extension25  max_y_extension25  \\\n",
            "0        -0.773254  ...           0.919237          -1.291872   \n",
            "1         0.691858  ...          -0.214399          -0.333294   \n",
            "2         0.447673  ...          -0.078833          -0.874162   \n",
            "3         0.447673  ...          -0.620679          -0.506091   \n",
            "4        -0.040698  ...          -1.364824          -0.216223   \n",
            "\n",
            "   mean_acc_on_paper25  mean_jerk_on_paper25  num_of_pendown25  paper_time25  \\\n",
            "0             1.808894              2.394641         -0.517888      0.226206   \n",
            "1             0.513445             -0.646723          1.978064     -0.239578   \n",
            "2            -1.191193             -1.241893         -0.388787      0.820635   \n",
            "3             0.658608              0.096405          1.719862     -0.239578   \n",
            "4             0.479906              0.601109          0.385819     -0.088198   \n",
            "\n",
            "   pressure_mean25  pressure_var25  total_time25  class  \n",
            "0         0.193248        2.443745      1.588992      P  \n",
            "1        -0.887902        2.128421     -0.537049      P  \n",
            "2        -1.212123       -0.311788     -0.043055      P  \n",
            "3        -1.060017        1.246315      2.500205      P  \n",
            "4         0.601921       -0.059682     -0.203572      P  \n",
            "\n",
            "[5 rows x 338 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_file.csv' with the actual path to your CSV file\n",
        "file_path = '/content/data_null_outlier_free_normalized_filtered_data.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the number of rows and columns\n",
        "num_rows, num_columns = df.shape\n",
        "print(\"Dataset after removing the highly correlated features (>90%)\")\n",
        "print(f\"Number of rows: {num_rows}\")\n",
        "print(f\"Number of columns: {num_columns}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORt97JNNyrAn",
        "outputId": "f791b605-15e9-4520-99d3-6f9f135883ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset after removing the highly correlated features (>90%)\n",
            "Number of rows: 174\n",
            "Number of columns: 338\n"
          ]
        }
      ]
    }
  ]
}